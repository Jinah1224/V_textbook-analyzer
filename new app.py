import streamlit as st
import pandas as pd
import re
from datetime import datetime, timedelta
import chardet
import time
from io import BytesIO
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager

# -------------------------------
# ì„¤ì •
# -------------------------------
st.set_page_config(page_title="ğŸ“š AI ê¸°ë°˜ êµê³¼ì„œ ê´€ë ¨ ë™í–¥ ë¶„ì„ê¸°", layout="wide")
st.title("ğŸ“š ì¹´ì¹´ì˜¤í†¡ ë¶„ì„ & ë‰´ìŠ¤ ìˆ˜ì§‘ í†µí•© ì•±")

# -------------------------------
# ê¸°ì¤€ ë°ì´í„°
# -------------------------------
kakao_categories = {
    "ì±„íƒ: ì„ ì • ê¸°ì¤€/í‰ê°€": ["í‰ê°€í‘œ", "ê¸°ì¤€", "ì¶”ì²œì˜ê²¬ì„œ", "ì„ ì •ê¸°ì¤€"],
    "ì±„íƒ: ìœ„ì›íšŒ ìš´ì˜": ["ìœ„ì›íšŒ", "í˜‘ì˜íšŒ", "ëŒ€í‘œêµì‚¬", "ìœ„ì›"],
    "ì±„íƒ: íšŒì˜/ì‹¬ì˜ ì§„í–‰": ["íšŒì˜", "íšŒì˜ë¡", "ì‹¬ì˜", "ì‹¬ì‚¬", "ìš´ì˜"],
    "ë°°ì†¡": ["ë°°ì†¡"],
    "ë°°ì†¡: ì§€ë„ì„œ/ì „ì‹œë³¸ ë„ì°©": ["ë„ì°©", "ì™”ì–´ìš”", "ì „ì‹œë³¸", "ì§€ë„ì„œ", "ë°•ìŠ¤"],
    "ë°°ì†¡: ë¼ë²¨/ì •ë¦¬ ì—…ë¬´": ["ë¼ë²¨", "ë¶„ë¥˜", "ì •ë¦¬", "ì „ì‹œ ì¤€ë¹„"],
    "ì£¼ë¬¸: ì‹œìŠ¤í…œ ì‚¬ìš©": ["ë‚˜ì´ìŠ¤", "ì—ë“€íŒŒì¸", "ë“±ë¡", "ì…ë ¥"],
    "ì£¼ë¬¸: ê³µë¬¸/ì •ì‚°": ["ê³µë¬¸", "ì •ì‚°", "ë§ˆê°ì¼", "ìš”ì²­"],
    "ì¶œíŒì‚¬: ìë£Œ ìˆ˜ë ¹/ì´ë²¤íŠ¸": ["ë³´ì¡°ìë£Œ", "ìë£Œ", "ê¸°í”„í‹°ì½˜", "ì´ë²¤íŠ¸"],
    "ì¶œíŒì‚¬: ìë£Œ íšŒìˆ˜/ìš”ì²­": ["íšŒìˆ˜", "ìš”ì²­", "êµì‚¬ìš©"]
}
publishers = ["ë¯¸ë˜ì—”", "ë¹„ìƒ", "ë™ì•„", "ì•„ì´ìŠ¤í¬ë¦¼", "ì²œì¬", "ì¢‹ì€ì±…", "ì§€í•™ì‚¬", "ëŒ€êµ", "ì´ë£¸", "ëª…ì§„", "ì²œì¬êµìœ¡"]
subjects = ["êµ­ì–´", "ìˆ˜í•™", "ì‚¬íšŒ", "ê³¼í•™", "ì˜ì–´", "ë„ë•", "ìŒì•…", "ë¯¸ìˆ ", "ì²´ìœ¡"]
complaint_keywords = ["ì•ˆ ì™”ì–´ìš”", "ì•„ì§", "ëŠ¦ê²Œ", "ì—†ì–´ìš”", "ì˜¤ë¥˜", "ë¬¸ì œ", "ì™œ", "í—·ê°ˆë ¤", "ë¶ˆí¸", "ì•ˆì˜´", "ì§€ì—°", "ì•ˆë³´ì—¬ìš”", "ëª» ë°›ì•˜", "í˜ë“¤ì–´ìš”"]

news_keywords = ["ì²œì¬êµìœ¡", "ì²œì¬êµê³¼ì„œ", "ì§€í•™ì‚¬", "ë²½í˜¸", "í”„ë¦°í”¼ì•„", "ë¯¸ë˜ì—”", "êµê³¼ì„œ", "ë™ì•„ì¶œíŒ"]
category_keywords = {
    "í›„ì›": ["í›„ì›", "ê¸°íƒ"], "ê¸°ë¶€": ["ê¸°ë¶€"], "í˜‘ì•½/MOU": ["í˜‘ì•½", "mou"],
    "ì—ë“€í…Œí¬/ë””ì§€í„¸êµìœ¡": ["ì—ë“€í…Œí¬", "ë””ì§€í„¸êµìœ¡", "aiêµìœ¡", "ìŠ¤ë§ˆíŠ¸êµìœ¡"],
    "ì •ì±…": ["ì •ì±…"], "ì¶œíŒ": ["ì¶œíŒ"], "ì¸ì‚¬/ì±„ìš©": ["ì±„ìš©", "êµì‚¬"],
    "í”„ë¦°íŠ¸ ë° ì¸ì‡„": ["ì¸ì‡„", "í”„ë¦°íŠ¸"], "ê³µê¸‰": ["ê³µê¸‰"], "êµìœ¡": ["êµìœ¡"], "ì´ë²¤íŠ¸": ["ì´ë²¤íŠ¸", "ì‚¬ì€í’ˆ"]
}

# -------------------------------
# ë‰´ìŠ¤ í¬ë¡¤ë§ (Selenium)
# -------------------------------
def crawl_news_selenium(keyword, pages=3):
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.5735.90 Safari/537.36")
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

    base_url = "https://search.naver.com/search.naver?where=news&query={query}&sort=1&nso=so:dd,p:2w&start={start}"
    results = []
    seen_links = set()

    for page in range(1, pages + 1):
        start = (page - 1) * 10 + 1
        driver.get(base_url.format(query=keyword, start=start))
        time.sleep(1)
        articles = driver.find_elements(By.CSS_SELECTOR, ".news_area")
        for a in articles:
            try:
                title_elem = a.find_element(By.CSS_SELECTOR, ".news_tit")
                title = title_elem.get_attribute("title")
                link = title_elem.get_attribute("href")
                if link in seen_links:
                    continue
                seen_links.add(link)
                summary_elem = a.find_element(By.CSS_SELECTOR, ".dsc_txt_wrap")
                summary = summary_elem.text if summary_elem else ""
                press = a.find_element(By.CSS_SELECTOR, ".info_group a").text
                full_text = (title + " " + summary).lower()
                results.append({
                    "ì¶œíŒì‚¬ëª…": check_publisher(full_text),
                    "ì¹´í…Œê³ ë¦¬": categorize_news(full_text),
                    "ë‚ ì§œ": "",  # ìƒëµìœ¼ë¡œ ì†ë„ â†‘
                    "ì œëª©": title,
                    "URL": link,
                    "ìš”ì•½": summary,
                    "ì–¸ë¡ ì‚¬": press,
                    "ë³¸ë¬¸ë‚´_êµê³¼ì„œ_ë˜ëŠ”_ë°œí–‰ì‚¬_ì–¸ê¸‰": "O" if "êµê³¼ì„œ" in full_text or "ë°œí–‰ì‚¬" in full_text else "X"
                })
            except:
                continue
    driver.quit()
    return pd.DataFrame(results)

def categorize_news(text):
    for cat, keywords in category_keywords.items():
        if any(k in text for k in keywords):
            return cat
    return "ê¸°íƒ€"

def check_publisher(text):
    for pub in news_keywords:
        if pub.replace(" ", "") in text.replace(" ", ""):
            return pub
    return "ê¸°íƒ€"

# -------------------------------
# ì¹´ì¹´ì˜¤í†¡ íŒŒì‹± í•¨ìˆ˜
# -------------------------------
def parse_kakao_text(text):
    parsed = []
    pattern1 = re.compile(r"(\d{4})ë…„ (\d{1,2})ì›” (\d{1,2})ì¼ (ì˜¤ì „|ì˜¤í›„)? (\d{1,2}):(\d{2}), (.+?) : (.+)")
    pattern2 = re.compile(r"\[(.*?)\] \[(ì˜¤ì „|ì˜¤í›„) (\d{1,2}):(\d{2})\] (.+)")
    date_pattern = re.compile(r"-+ (\d{4})ë…„ (\d{1,2})ì›” (\d{1,2})ì¼")
    lines = text.splitlines()
    current_date = None
    for line in lines:
        if m1 := pattern1.match(line):
            y, m, d, ampm, h, mi, sender, msg = m1.groups()
            h = int(h)
            mi = int(mi)
            if ampm == "ì˜¤í›„" and h != 12:
                h += 12
            elif ampm == "ì˜¤ì „" and h == 12:
                h = 0
            dt = datetime(int(y), int(m), int(d), h, mi)
            if sender.strip() != "ì˜¤í”ˆì±„íŒ…ë´‡":
                parsed.append({"ë‚ ì§œ": dt.date(), "ì‹œê°„": dt.time(), "ë³´ë‚¸ ì‚¬ëŒ": sender.strip(), "ë©”ì‹œì§€": msg.strip()})
        elif m2 := pattern2.match(line):
            sender, ampm, h, mi, msg = m2.groups()
            if current_date:
                h = int(h)
                mi = int(mi)
                if ampm == "ì˜¤í›„" and h != 12:
                    h += 12
                elif ampm == "ì˜¤ì „" and h == 12:
                    h = 0
                t = datetime.strptime(f"{h}:{mi}", "%H:%M").time()
                parsed.append({"ë‚ ì§œ": current_date, "ì‹œê°„": t, "ë³´ë‚¸ ì‚¬ëŒ": sender.strip(), "ë©”ì‹œì§€": msg.strip()})
        elif d := date_pattern.match(line):
            y, m, d = map(int, d.groups())
            current_date = datetime(y, m, d).date()
    return pd.DataFrame(parsed)

def classify_category(text):
    for cat, keywords in kakao_categories.items():
        if any(w in text for w in keywords):
            return cat
    return "ê¸°íƒ€"

def extract_kakao_publisher(text):
    for pub in publishers:
        if pub in text:
            return pub
    return None

def extract_subject(text):
    for sub in subjects:
        if sub in text:
            return sub
    return None

def detect_complaint(text):
    return "O" if any(k in text for k in complaint_keywords) else "X"

# -------------------------------
# Streamlit UI
# -------------------------------
tab1, tab2 = st.tabs(["ğŸ’¬ ì¹´ì¹´ì˜¤í†¡ ë¶„ì„", "ğŸ“° ë‰´ìŠ¤ ìˆ˜ì§‘"])

with tab1:
    st.subheader("ì¹´ì¹´ì˜¤í†¡ .txt ì—…ë¡œë“œ")
    uploaded = st.file_uploader("ì¹´ì¹´ì˜¤í†¡ ëŒ€í™” í…ìŠ¤íŠ¸ íŒŒì¼ ì—…ë¡œë“œ", type="txt")
    if uploaded:
        raw_bytes = uploaded.read()
        encoding = chardet.detect(raw_bytes)["encoding"] or "utf-8"
        text = raw_bytes.decode(encoding, errors="ignore")
        df_kakao = parse_kakao_text(text)
        if df_kakao.empty:
            st.warning("â— ë©”ì‹œì§€ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            df_kakao["ì¹´í…Œê³ ë¦¬"] = df_kakao["ë©”ì‹œì§€"].apply(classify_category)
            df_kakao["ì¶œíŒì‚¬"] = df_kakao["ë©”ì‹œì§€"].apply(extract_kakao_publisher)
            df_kakao["ê³¼ëª©"] = df_kakao["ë©”ì‹œì§€"].apply(extract_subject)
            df_kakao["ë¶ˆë§Œ ì—¬ë¶€"] = df_kakao["ë©”ì‹œì§€"].apply(detect_complaint)
            st.success(f"âœ… ì´ {len(df_kakao)}ê°œ ë©”ì‹œì§€ ë¶„ì„ ì™„ë£Œ!")
            st.dataframe(df_kakao)
            csv = df_kakao.to_csv(index=False).encode("utf-8")
            st.download_button("ğŸ“¥ ì¹´ì¹´ì˜¤í†¡ CSV ì €ì¥", csv, "kakao_cleaned.csv", "text/csv")

with tab2:
    st.subheader("ì¶œíŒì‚¬ ê´€ë ¨ ë‰´ìŠ¤ í¬ë¡¤ë§ (ìµœê·¼ 2ì£¼)")
    selected_keywords = st.multiselect("ğŸ” ê¸°ë³¸ í‚¤ì›Œë“œ ì„ íƒ", news_keywords, default=news_keywords)
    if selected_keywords and st.button("ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘"):
        progress = st.progress(0)
        all_news = []
        for i, kw in enumerate(selected_keywords):
            df = crawl_news_selenium(kw)
            all_news.append(df)
            progress.progress((i+1)/len(selected_keywords))
        df_news = pd.concat(all_news, ignore_index=True)
        st.success("âœ… ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ!")
        st.dataframe(df_news)
        buffer = BytesIO()
        with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer:
            df_news.to_excel(writer, index=False, sheet_name="ë‰´ìŠ¤ê²°ê³¼")
        st.download_button("ğŸ“¥ ë‰´ìŠ¤ ì—‘ì…€ ì €ì¥", buffer.getvalue(), "news_result.xlsx", "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
